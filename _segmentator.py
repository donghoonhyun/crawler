# encoding=utf-8
import jieba.analyse
import pandas
import re
from _connectdb import *


def segmenter(amode, pmode, doc_info, topk, stop_words):
    targetid = doc_info[0]
    url = doc_info[1]
    doc = doc_info[2]
    doc = preprocess(doc)

    if amode == 'F':
        seg_list = jieba.lcut(doc, cut_all=True)
        if pmode == 'P':
            print("Full Mode:")
            print("/ ".join(seg_list))
        elif pmode == 'D':
            print(seg_list)

    elif amode == 'D':
        seg_list = jieba.lcut(doc, cut_all=False)
        if pmode == 'P':
            print("Default Mode:")
            print("/ ".join(seg_list))
        elif pmode == 'D':
            process(targetid, url, seg_list, stop_words)

    elif amode == 'S':
        seg_list = jieba.lcut_for_search(doc)
        if pmode == 'P':
            print("Search Mode:")
            print("/ ".join(seg_list))
        elif pmode == 'D':
            print(seg_list)

    elif amode == 'E':
        # jieba.analyse.set_stop_words("./extra_dict/stop_words.txt")
        seg_list = jieba.analyse.extract_tags(doc, topK=topk, withWeight=False, allowPOS=())
        if pmode == 'P':
            print("Extract Mode:")
            print("/ ".join(seg_list))
        elif pmode == 'D':
            process(targetid, url, seg_list, stop_words)

    elif amode == 'A':
        seg_list = jieba.cut(doc, cut_all=True)
        if pmode == 'P':
            print("Full Mode:")
            print("/ ".join(seg_list))
            print("")
        elif pmode == 'D':
            print(seg_list)

        seg_list = jieba.cut(doc, cut_all=False)
        if pmode == 'P':
            print("Default Mode:")
            print("/ ".join(seg_list))
            print("")
        elif pmode == 'D':
            print(seg_list)

        seg_list = jieba.cut_for_search(doc)
        if pmode == 'P':
            print("Search Mode:")
            print("/ ".join(seg_list))
            print("")
        elif pmode == 'D':
            print(seg_list)

        # jieba.analyse.set_stop_words("./extra_dict/stop_words.txt")
        seg_list = jieba.analyse.extract_tags(doc, topK=topk, withWeight=False, allowPOS=())
        if pmode == 'P':
            print("Extract Mode:")
            print("/ ".join(seg_list))
        elif pmode == 'D':
            print(seg_list)


def preprocess(doc):
    # remove blank
    doc = doc.replace('  ', ' ').replace('-', ' ').replace('â€“', ' ').replace('+', ' ').replace('*', ' ')
    doc = doc.replace('!', ' ').replace('{', ' ').replace('}', ' ').replace('ï¿£', ' ')
    doc = doc.replace('(', ' ').replace(')', ' ').replace(',', ' ').replace('?', ' ')
    doc = doc.replace('ï¼©', ' ').replace('|', ' ').replace('ï¼©', ' ')
    doc = doc.replace('[', ' ').replace(']', ' ').replace('â€™', ' ').replace('â€œ', ' ').replace('â€', ' ')
    doc = doc.replace('\r', '').replace('\t', '').replace('\n', '').replace('[\b]', '').replace('~', ' ')
    # remove special symbol
    doc = re.sub('[ï¼š:;ï¼›/ã€Šã€‹<>ï¼ˆï¼‰ã€ã€‘ã€Œã€â•°â•¯â•­â•®ã€ï¼Œã€‚Â°ãƒ»Â·,.`Â´â€œâ€â€˜â€™"Ã—ï¼Ÿï¼^%~ï¼=â€¦ï¼†&#@\d\s]', ' ', doc)
    doc = re.sub('[â™¥â¤â™£â™¡â˜˜â˜»â˜Ÿâ˜â˜•â˜‰â­â˜†â˜…â—¡â—•â—â—‹â—‰â—‡â–½â–¼â–¶â–³â–²â–¡â– â”»â”‘â”â”€â‘¨_Ã˜Ê–Ë†Ë‡Ë˜Ë™ËµË¶ÎŸÎ Î£Î©]', ' ', doc)
    doc = re.sub('[Ğ”Ğ—Ô„ØŒâ€”â†“â†‘â†â†’Ø¤Ø£Ø¡â…¡âˆ€âˆ‡â„ƒâ€¢â€»â€¿OÃ”âˆšâ‘ â‘¡â‘¢ã… ï¼¡ï¼¢ğŸ˜„ğŸ˜¡ğŸ˜‚â›³ï¸]', ' ', doc)
    # change all english character to upper character
    doc = doc.lower()

    # jieba word suggest
    jieba.suggest_freq('exo', True)
    jieba.suggest_freq('dancing king', True)
    jieba.suggest_freq('æ— é™æŒ‘æˆ˜', True)
    jieba.suggest_freq('åˆ˜åœ¨çŸ³', True)

    return doc


def remove_stop_words(df, stop_words):
    tmp_list = []
    tmp_dict = {}
    for i in range(len(df)):
        if df['word'].loc[i] in stop_words:
            pass
        elif df['word'].loc[i] == ' ':
            pass
        else:
            tmp_dict = {'word': df['word'].loc[i], 'count': df['count'].loc[i]}
            tmp_list.append(tmp_dict)
    df = pandas.DataFrame(tmp_list, columns=['word', 'count'])
    return df


def process(targetid, url, seg_list, stop_words):
    frame = pandas.DataFrame(seg_list, columns=['word'])
    gframe = frame.groupby(['word']).size().reset_index(name='count')
    gframe = remove_stop_words(gframe, stop_words)
    gframe['targetID'] = targetid
    gframe['url'] = url
    save_db(targetid, url, gframe)


def save_db(targetid, url, df):
    save_word(targetid, url, df)

